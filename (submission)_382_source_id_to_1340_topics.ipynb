{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtMXuVA2KUE7xFhxSw13hs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtCZhEXvdYMA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fetch representative topics for OpenAlex sources (as returned by the API).\n",
        "\n",
        "Input : a text file with one normalized OpenAlex source ID per line (e.g., S1234567890)\n",
        "Output: CSV (long format):\n",
        "        source_id, source_title, topic_rank, topic_id, topic_name, count\n",
        "\n",
        "Usage:\n",
        "  python openalex_source_topics.py input_source_ids.txt output_topics_long.csv\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from urllib.parse import urlencode\n",
        "\n",
        "BASE_URL = \"https://api.openalex.org\"\n",
        "USER_AGENT = \"openalex-source-topics-fetcher/1.0\"\n",
        "SELECT_FIELDS = \"display_name,topics\"\n",
        "\n",
        "\n",
        "def read_source_ids(path: str) -> list[str]:\n",
        "    \"\"\"Read already-normalized source IDs (one per line).\"\"\"\n",
        "    ids = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s:\n",
        "                ids.append(s)\n",
        "\n",
        "    # remove duplicates while preserving order\n",
        "    seen, uniq = set(), []\n",
        "    for x in ids:\n",
        "        if x not in seen:\n",
        "            seen.add(x)\n",
        "            uniq.append(x)\n",
        "\n",
        "    print(f\"[INFO] loaded: {len(uniq)} unique source IDs\")\n",
        "    return uniq\n",
        "\n",
        "\n",
        "def req_json(url: str, max_retries: int = 5, backoff: float = 1.6) -> dict:\n",
        "    \"\"\"GET JSON with retry for transient errors.\"\"\"\n",
        "    last_err = None\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=30)\n",
        "            if r.status_code == 200:\n",
        "                return r.json()\n",
        "            if r.status_code in (429, 500, 502, 503, 504):\n",
        "                time.sleep(backoff ** i)\n",
        "                continue\n",
        "            r.raise_for_status()\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            time.sleep(backoff ** i)\n",
        "    raise RuntimeError(f\"Request failed after {max_retries} retries: {last_err}\")\n",
        "\n",
        "\n",
        "def fetch_topics_for_source(sid: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Fetch topics exactly as returned by OpenAlex for a given source.\n",
        "    \"\"\"\n",
        "    url = f\"{BASE_URL}/sources/{sid}?{urlencode({'select': SELECT_FIELDS})}\"\n",
        "    data = req_json(url)\n",
        "\n",
        "    source_title = data.get(\"display_name\", \"\")\n",
        "    topics = data.get(\"topics\") or []\n",
        "\n",
        "    rows = []\n",
        "    for rank, t in enumerate(topics, start=1):\n",
        "        rows.append({\n",
        "            \"source_id\": sid,\n",
        "            \"source_title\": source_title,\n",
        "            \"topic_rank\": rank,\n",
        "            \"topic_id\": t.get(\"id\", \"\"),\n",
        "            \"topic_name\": t.get(\"display_name\", \"\"),\n",
        "            \"count\": t.get(\"count\"),\n",
        "        })\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "def fetch_all(source_ids: list[str], sleep_sec: float = 0.15) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    failed = []\n",
        "\n",
        "    for i, sid in enumerate(source_ids, start=1):\n",
        "        try:\n",
        "            rows.extend(fetch_topics_for_source(sid))\n",
        "        except Exception as e:\n",
        "            failed.append((sid, str(e)))\n",
        "\n",
        "        time.sleep(sleep_sec)\n",
        "\n",
        "        if i % 25 == 0:\n",
        "            print(f\"[INFO] {i}/{len(source_ids)} processed | rows={len(rows)}\")\n",
        "\n",
        "    if failed:\n",
        "        print(f\"[WARN] failed: {len(failed)} sources (showing up to 3): {failed[:3]}\")\n",
        "\n",
        "    return pd.DataFrame(rows, columns=[\n",
        "        \"source_id\", \"source_title\", \"topic_rank\", \"topic_id\", \"topic_name\", \"count\"\n",
        "    ])\n",
        "\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python openalex_source_topics.py input_source_ids.txt output.csv\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    in_path = sys.argv[1]\n",
        "    out_path = sys.argv[2]\n",
        "\n",
        "    source_ids = read_source_ids(in_path)\n",
        "    df = fetch_all(source_ids)\n",
        "\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"[SAVE] {out_path} (rows={len(df)})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}