{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH8JNSZU6a7raQLqYMRPMN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwW5LTQbW5Nd"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "OpenAlex: Top-50 topics → journals via primary-topic works,\n",
        "then filter journals whose 25 representative topics include ≥1 of the Top-50.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set, Iterable, Optional, Any\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "BASE = \"https://api.openalex.org\"\n",
        "\n",
        "MAILTO = os.getenv(\"OPENALEX_MAILTO\", \"YOUR_EMAIL@example.com\")\n",
        "BASE_DIR = os.getenv(\"OPENALEX_BASE_DIR\", os.path.join(\".\", \"data\"))\n",
        "SAVE_DIR = os.path.join(BASE_DIR, \"out_primary_Representative_topic25\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Retries / throttling\n",
        "MAX_RETRIES = int(os.getenv(\"OPENALEX_MAX_RETRIES\", \"6\"))\n",
        "BACKOFF = float(os.getenv(\"OPENALEX_BACKOFF\", \"1.6\"))\n",
        "TIMEOUT_S = int(os.getenv(\"OPENALEX_TIMEOUT_S\", \"60\"))\n",
        "\n",
        "# Threshold\n",
        "MIN_TOPIC_COUNT = int(os.getenv(\"OPENALEX_MIN_TOPIC_COUNT\", \"1\"))\n",
        "\n",
        "# Checkpoints\n",
        "CP_TID2SIDS = os.path.join(SAVE_DIR, \"cp_tid_to_sids_primary.json\")   # {tid: [sid,...]}\n",
        "CP_SID_TOP25 = os.path.join(SAVE_DIR, \"cp_sid_top25_topics.json\")     # {sid: [tid,...]}\n",
        "\n",
        "# Outputs\n",
        "OUT_SUMMARY_TPL = os.path.join(SAVE_DIR, \"journals_Representative_topic25_summary_{}.csv\")\n",
        "OUT_LONG_TPL   = os.path.join(SAVE_DIR, \"journals_Representative_topic25_long_{}.csv\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Input: Top-50 topic IDs\n",
        "# -----------------------------\n",
        "TOPIC_ID_CSV = \"\"\"T10712 , T12289, T12863, T11813, T11657, T10102, T10068, T14330, T12028, T11937, T11891, T10215, T10609, T12377, T10557,\n",
        "T10162 , T13166, T13516, T10953, T14386, T11499, T11572, T12171, T11744,\n",
        "T10003, T11197, T11024, T11719, T14246, T12364, T10355, T10286, T11530,\n",
        "T11986 , T13807, T10181, T13083, T11045, T14109, T10028, T12016, T12478,\n",
        "T11437 , T11147, T14380, T13607, T12573, T13496, T10154, T12764\"\"\"\n",
        "TOPIC_IDS: List[str] = re.findall(r\"T\\d+\", TOPIC_ID_CSV)\n",
        "TOPIC_SET: Set[str] = set(TOPIC_IDS)\n",
        "\n",
        "# -----------------------------\n",
        "# HTTP session\n",
        "# -----------------------------\n",
        "SESSION = requests.Session()\n",
        "SESSION.headers.update({\"User-Agent\": \"openalex-lis-top50-journals/1.0\"})\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def req_json(url: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "    \"\"\"GET JSON with retries; always appends mailto.\"\"\"\n",
        "    params = dict(params or {})\n",
        "    params[\"mailto\"] = MAILTO\n",
        "\n",
        "    last_err = None\n",
        "    for i in range(MAX_RETRIES):\n",
        "        try:\n",
        "            r = SESSION.get(url, params=params, timeout=TIMEOUT_S)\n",
        "            if r.status_code == 200:\n",
        "                return r.json()\n",
        "            last_err = f\"{r.status_code} {r.text[:200]}\"\n",
        "        except Exception as e:\n",
        "            last_err = str(e)\n",
        "        time.sleep(BACKOFF ** (i + 1))\n",
        "    raise RuntimeError(f\"GET failed: {url} :: {last_err}\")\n",
        "\n",
        "\n",
        "def save_json(path: str, obj: Any) -> None:\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "def load_json(path: str) -> Optional[Any]:\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    return None\n",
        "\n",
        "\n",
        "def to_sid(openalex_id_or_sid: str) -> str:\n",
        "    \"\"\"Normalize to 'Sxxxx'.\"\"\"\n",
        "    s = str(openalex_id_or_sid).strip()\n",
        "    if s.startswith(\"https://openalex.org/\"):\n",
        "        s = s.rsplit(\"/\", 1)[-1]\n",
        "    return s\n",
        "\n",
        "\n",
        "def to_tid(openalex_id_or_tid: str) -> str:\n",
        "    \"\"\"Normalize to 'Txxxx'.\"\"\"\n",
        "    t = str(openalex_id_or_tid).strip()\n",
        "    if t.startswith(\"https://openalex.org/\"):\n",
        "        t = t.rsplit(\"/\", 1)[-1]\n",
        "    return t\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: topic → works → journals (primary topic only, journal sources only)\n",
        "# -----------------------------\n",
        "def collect_journal_sids_from_primary_topic(tid: str) -> Set[str]:\n",
        "    sids: Set[str] = set()\n",
        "    cursor = \"*\"\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"filter\": f\"primary_topic.id:{tid},primary_location.source.type:journal\",\n",
        "            \"select\": \"id,primary_location\",\n",
        "            \"per-page\": 200,\n",
        "            \"cursor\": cursor,\n",
        "        }\n",
        "        js = req_json(f\"{BASE}/works\", params=params)\n",
        "        results = js.get(\"results\", [])\n",
        "\n",
        "        for w in results:\n",
        "            src = (w.get(\"primary_location\") or {}).get(\"source\") or {}\n",
        "            if src.get(\"type\") == \"journal\":\n",
        "                sid = to_sid(src.get(\"id\", \"\"))\n",
        "                if sid.startswith(\"S\"):\n",
        "                    sids.add(sid)\n",
        "\n",
        "        cursor = js.get(\"meta\", {}).get(\"next_cursor\")\n",
        "        if not cursor or not results:\n",
        "            break\n",
        "\n",
        "    return sids\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: journal → 25 representative topics (batch, consistent with UI)\n",
        "# -----------------------------\n",
        "def fetch_sources_top25_topics_batch(sids: Iterable[str]) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Batch call to /sources with filter=openalex:S...|S... (up to 200 per call).\n",
        "    Returns {sid: [T...]} for sources that returned topics (empty topics omitted).\n",
        "    \"\"\"\n",
        "    out: Dict[str, List[str]] = {}\n",
        "    ids = [to_sid(s) for s in sids]\n",
        "\n",
        "    for i in range(0, len(ids), 200):\n",
        "        chunk = ids[i:i + 200]\n",
        "        if not chunk:\n",
        "            continue\n",
        "\n",
        "        js = req_json(\n",
        "            f\"{BASE}/sources\",\n",
        "            params={\n",
        "                \"filter\": \"openalex:\" + \"|\".join(chunk),\n",
        "                \"select\": \"id,topics\",\n",
        "                \"per-page\": 200,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        for r in js.get(\"results\", []):\n",
        "            sid = to_sid(r.get(\"id\", \"\"))\n",
        "            topics = r.get(\"topics\") or []\n",
        "            tids = [to_tid(t.get(\"id\", \"\")) for t in topics if to_tid(t.get(\"id\", \"\")).startswith(\"T\")]\n",
        "            if tids:\n",
        "                out[sid] = tids\n",
        "\n",
        "        time.sleep(0.1)  # polite\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def fetch_source_top25_topics_single(sid: str) -> List[str]:\n",
        "    \"\"\"Fallback: /sources/{sid}?select=topics (omits empty).\"\"\"\n",
        "    js = req_json(f\"{BASE}/sources/{sid}\", params={\"select\": \"topics\"})\n",
        "    topics = js.get(\"topics\") or []\n",
        "    tids = [to_tid(t.get(\"id\", \"\")) for t in topics if to_tid(t.get(\"id\", \"\")).startswith(\"T\")]\n",
        "    return tids\n",
        "\n",
        "\n",
        "def hydrate_top25_cache(all_sids: List[str], cache: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
        "    \"\"\"Fill cache via batch; if still missing, fallback to single calls.\"\"\"\n",
        "    missing = [sid for sid in all_sids if sid not in cache]\n",
        "    if missing:\n",
        "        cache.update(fetch_sources_top25_topics_batch(missing))\n",
        "\n",
        "    # If still missing (e.g., transient errors), try single fallback\n",
        "    still_missing = [sid for sid in all_sids if sid not in cache]\n",
        "    for sid in still_missing:\n",
        "        try:\n",
        "            tids = fetch_source_top25_topics_single(sid)\n",
        "            if tids:\n",
        "                cache[sid] = tids\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] failed source {sid}: {e}\")\n",
        "\n",
        "    return cache\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Meta: sources metadata (for output)\n",
        "# -----------------------------\n",
        "def batch_fetch_sources_meta(sids: List[str]) -> Dict[str, Dict[str, Any]]:\n",
        "    out: Dict[str, Dict[str, Any]] = {}\n",
        "    ids = [to_sid(s) for s in sids]\n",
        "\n",
        "    for i in range(0, len(ids), 50):\n",
        "        chunk = ids[i:i + 50]\n",
        "        if not chunk:\n",
        "            continue\n",
        "\n",
        "        js = req_json(\n",
        "            f\"{BASE}/sources\",\n",
        "            params={\n",
        "                \"filter\": \"openalex:\" + \"|\".join(chunk),\n",
        "                \"select\": \"id,display_name,issn_l,issn,host_organization_name,country_code,type\",\n",
        "                \"per-page\": 200,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        for r in js.get(\"results\", []):\n",
        "            out[to_sid(r.get(\"id\", \"\"))] = r\n",
        "\n",
        "        time.sleep(0.4)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main pipeline (steps 1–5)\n",
        "# -----------------------------\n",
        "def main(min_topics: int = MIN_TOPIC_COUNT) -> None:\n",
        "    if not TOPIC_IDS:\n",
        "        raise RuntimeError(\"No topic IDs provided.\")\n",
        "\n",
        "    # Step 1: For each Top-50 topic, collect journal SIDs from works with that primary topic\n",
        "    cp_tid2sids: Dict[str, List[str]] = load_json(CP_TID2SIDS) or {}\n",
        "    for idx, tid in enumerate(TOPIC_IDS, 1):\n",
        "        if tid in cp_tid2sids:\n",
        "            print(f\"[{idx}/{len(TOPIC_IDS)}] {tid} cached ({len(cp_tid2sids[tid])} sids)\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            sids = sorted(collect_journal_sids_from_primary_topic(tid))\n",
        "            cp_tid2sids[tid] = sids\n",
        "            print(f\"[{idx}/{len(TOPIC_IDS)}] {tid}: {len(sids)} sids (primary works)\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] topic {tid} failed: {e}\")\n",
        "            cp_tid2sids[tid] = []\n",
        "\n",
        "        save_json(CP_TID2SIDS, cp_tid2sids)\n",
        "        if idx % 3 == 0:\n",
        "            time.sleep(0.5)\n",
        "\n",
        "    # Step 2 (prep): Build source → matched Top-50 topics from Step 1 results\n",
        "    source_to_topics: Dict[str, Set[str]] = defaultdict(set)\n",
        "    for tid, sids in cp_tid2sids.items():\n",
        "        for sid in sids:\n",
        "            source_to_topics[to_sid(sid)].add(tid)\n",
        "\n",
        "    all_sids = list(source_to_topics.keys())\n",
        "\n",
        "    # Step 3: Fetch each journal's 25 representative topics (UI-consistent) and retain journals\n",
        "    sid_top25_cache: Dict[str, List[str]] = load_json(CP_SID_TOP25) or {}\n",
        "    sid_top25_cache = hydrate_top25_cache(all_sids, sid_top25_cache)\n",
        "    save_json(CP_SID_TOP25, sid_top25_cache)\n",
        "\n",
        "    filtered: Dict[str, Set[str]] = {}\n",
        "    for sid, matched_top50 in source_to_topics.items():\n",
        "        top25 = set(sid_top25_cache.get(sid, []))\n",
        "        # Keep journal if (Top-50 matched via works) ∩ (25 representative topics) is non-empty\n",
        "        keep = matched_top50.intersection(top25).intersection(TOPIC_SET)\n",
        "        if keep:\n",
        "            filtered[sid] = keep\n",
        "\n",
        "    # Step 4: Apply threshold (top50_1 .. top50_k idea corresponds to min_topics)\n",
        "    kept = [(sid, len(tset), sorted(tset)) for sid, tset in filtered.items() if len(tset) >= int(min_topics)]\n",
        "    kept.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if not kept:\n",
        "        print(f\"[DONE] No journals found with ≥{int(min_topics)} topics.\")\n",
        "        return\n",
        "\n",
        "    # Step 5: Output\n",
        "    meta = batch_fetch_sources_meta([sid for sid, _, _ in kept])\n",
        "\n",
        "    rows = []\n",
        "    long_rows = []\n",
        "\n",
        "    for sid, cnt, tids in kept:\n",
        "        m = meta.get(sid, {})\n",
        "        rows.append(\n",
        "            {\n",
        "                \"source_id\": f\"https://openalex.org/{sid}\",\n",
        "                \"display_name\": m.get(\"display_name\", \"\"),\n",
        "                \"issn_l\": m.get(\"issn_l\", \"\"),\n",
        "                \"issn\": \";\".join(m.get(\"issn\", []) or []),\n",
        "                \"publisher\": m.get(\"host_organization_name\", \"\"),\n",
        "                \"country_code\": m.get(\"country_code\", \"\"),\n",
        "                \"type\": m.get(\"type\", \"\"),\n",
        "                \"topic_match_count\": cnt,            # guaranteed ≤ 25\n",
        "                \"matched_topic_ids\": \";\".join(tids), # Top-25 ∩ Top-50\n",
        "            }\n",
        "        )\n",
        "\n",
        "        for t in tids:\n",
        "            long_rows.append(\n",
        "                {\n",
        "                    \"source_id\": f\"https://openalex.org/{sid}\",\n",
        "                    \"display_name\": m.get(\"display_name\", \"\"),\n",
        "                    \"topic_id\": t,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    df_summary = pd.DataFrame(rows)\n",
        "    df_long = pd.DataFrame(long_rows)\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out_summary = OUT_SUMMARY_TPL.format(ts)\n",
        "    out_long = OUT_LONG_TPL.format(ts)\n",
        "\n",
        "    df_summary.to_csv(out_summary, index=False, encoding=\"utf-8-sig\")\n",
        "    df_long.to_csv(out_long, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"[DONE] Journals with ≥{int(min_topics)} topics: {len(df_summary)}\")\n",
        "    print(\"Saved CSV (summary):\", out_summary)\n",
        "    print(\"Saved CSV (pairs):\", out_long)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(min_topics=MIN_TOPIC_COUNT)"
      ]
    }
  ]
}